{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8c6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SmallFilesContainer import SmallFilesContainer\n",
    "from MapReduceEngine import MapReduceEngine\n",
    "from VirtualBigFile import *\n",
    "\n",
    "# general\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from io import StringIO\n",
    "\n",
    "# ml\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook\n",
    "from IPython.display import display\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6135a57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be10315",
   "metadata": {},
   "outputs": [],
   "source": [
    "smallFilesContainer = SmallFilesContainer(\"MapReduceSmallFiles.csv\")\n",
    "\n",
    "smallFilesContainer.deleteAllFiles()\n",
    "\n",
    "def get_input_filename(i:int):\n",
    "    return \"my_input_file_{:05d}.csv\".format(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7894a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDatasets(num_files=100000, rows_in_file=10):\n",
    "    print(\"Creating {} input files. Each file contains {} rows. Each row contains: firstname,city,secondname\"\\\n",
    "          .format(num_files,rows_in_file))\n",
    "    firstname  = ['John', 'Dana', 'Scott', 'Marc', 'Steven', 'Michael', 'Albert', 'Johanna']\n",
    "    city       = ['NewYork', 'Haifa', 'Munchen', 'London', 'PaloAlto',  'TelAviv', 'Kiel', 'Hamburg']\n",
    "    secondname = ['Smith', 'Brown', 'Miller', 'Watson', 'Bain']\n",
    "\n",
    "    filenames = []\n",
    "    for i in tqdm(range(num_files)):\n",
    "        first     = np.random.choice(a=firstname,  size=rows_in_file)\n",
    "        cit       = np.random.choice(a=city,       size=rows_in_file)\n",
    "        second    = np.random.choice(a=secondname, size=rows_in_file)\n",
    "        df        = pd.DataFrame({'firstname': first, 'city': cit, 'secondname': second})\n",
    "        file_name = get_input_filename(i)\n",
    "        filenames.append(file_name)\n",
    "        smallFilesContainer.createNewFile(file_name,df.to_csv(index=False, header=True),deleteExist=True)\n",
    "    return filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd77afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 100000 input files. Each file contains 10 rows. Each row contains: firstname,city,secondname\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 100000/100000 [01:01<00:00, 1626.29it/s]\n"
     ]
    }
   ],
   "source": [
    "filenames = createDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd5aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_csv(filename:str, delete:bool, header:bool):\n",
    "    tuples  = smallFilesContainer.readFile(filename, type_=[tuple])\n",
    "    if delete:\n",
    "        smallFilesContainer.deleteFiles(filename)\n",
    "    return pd.DataFrame(tuples[1:],columns=tuples[0]) if header else pd.DataFrame(tuples)\n",
    "\n",
    "def map_output_filename(threadID: int):\n",
    "    return \"map-output-{}.csv\".format(threadID)\n",
    "\n",
    "def map_process(threadID, input_filenames):\n",
    "    tuples = [('key', 'value')]\n",
    "    for filename in input_filenames:\n",
    "        data = read_df_from_csv(filename, delete=False,header=True)\n",
    "        # iterate through different columns to find location of each key-value pair\n",
    "        for col in data.columns:\n",
    "            tuples.extend([(col + '_' + value, filename) for value in data[col].values])\n",
    "    # still using big files because each thread can create a huge partition that consists from many small input files\n",
    "    output_filename = map_output_filename(threadID)\n",
    "    outputFile = VirtualBigFile(output_filename)\n",
    "    outputFile.delete()\n",
    "    outputFile.append(tuples)\n",
    "    outputFile.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cfdc7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_read_temp_from_input(threadID):\n",
    "    filename = map_output_filename(threadID)\n",
    "    bigFile  = VirtualBigFile(filename)\n",
    "    tuples   = bigFile.readData(type_=[tuple])\n",
    "    bigFile.delete()\n",
    "    return pd.DataFrame(tuples[1:],columns=tuples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ffc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_process(threadID, shuffle_rows):\n",
    "    tuples = []\n",
    "    for shuffle_row in shuffle_rows:\n",
    "        value, documents = shuffle_row[0], shuffle_row[1]\n",
    "        '''This function takes a value pair and its locations and places them in alist without duplicates'''\n",
    "        #split docs into list and set them to to remove duplicates\n",
    "        docs = sorted(list(set(documents.split(','))))\n",
    "        #generate output list\n",
    "        tuples.append((value, ':'.join(docs)))\n",
    "    # still using big files because each thread can create a huge partition that consists from many small input files\n",
    "    output_filename = \"part-{}-final.csv\".format(threadID)\n",
    "    outputFile = VirtualBigFile(output_filename)\n",
    "    outputFile.delete()\n",
    "    outputFile.append(tuples)\n",
    "    outputFile.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf95236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Map stage with 100000 input objects splitted to 8 threads...\n",
      "Map thread 0 is starting with 12500 objects ...\n",
      "Map thread 1 is starting with 12500 objects ...\n",
      "Map thread 2 is starting with 12500 objects ...\n",
      "Map thread 3 is starting with 12500 objects ...\n",
      "Map thread 4 is starting with 12500 objects ...\n",
      "Map thread 5 is starting with 12500 objects ...\n",
      "Map thread 6 is starting with 12500 objects ...\n",
      "Map thread 7 is starting with 12500 objects ...\n",
      "Map thread 0 is completed\n",
      "Map thread 6 is completed\n",
      "Map thread 2 is completed\n",
      "Map thread 3 is completed\n",
      "Map thread 1 is completed\n",
      "Map thread 5 is completed\n",
      "Map thread 7 is completed\n",
      "Map thread 4 is completed\n",
      "Map stage completed in 50.90843677520752 seconds.\n",
      "Starting Reduce stage with 21 input objects splitted to 8 threads...\n",
      "Reduce thread 0 is starting with 3 objects ...\n",
      "Reduce thread 1 is starting with 3 objects ...\n",
      "Reduce thread 2 is starting with 3 objects ...\n",
      "Reduce thread 3 is starting with 3 objects ...\n",
      "Reduce thread 4 is starting with 3 objects ...\n",
      "Reduce thread 5 is starting with 2 objects ...\n",
      "Reduce thread 6 is starting with 2 objects ...\n",
      "Reduce thread 7 is starting with 2 objects ...\n",
      "Reduce thread 0 is completed\n",
      "Reduce thread 1 is completed\n",
      "Reduce thread 7 is completed\n",
      "Reduce thread 3 is completed\n",
      "Reduce thread 2 is completed\n",
      "Reduce thread 5 is completed\n",
      "Reduce thread 4 is completed\n",
      "Reduce thread 6 is completed\n",
      "Reduce stage completed in 1.2888920307159424 seconds.\n",
      "MapReduce Completed in 64.06488513946533 seconds.\n"
     ]
    }
   ],
   "source": [
    "MapReduceEngine.execute(filenames, map_process, shuffle_read_temp_from_input, reduce_process, max_threads=8)\n",
    "\n",
    "smallFilesContainer.flush(objectStorageFlush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd29a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VirtualBigFile.deleteFiles(filenames)\n",
    "#VirtualBigFile.flushFiles(filenames, objectStorageFlush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
